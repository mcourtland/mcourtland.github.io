% NLP 101
% Maury Courtland (www.maury.science)
% June 11, 2019
---
author:
- Maury Courtland
description:
- An introduction to NLP for the Triangle Machine Learning and Computer Vision Group
title:
- NLP 101
---



## Talk guidelines
- Everything is at a fairly high level
    - If you're interested in learning more, please ask
    - If it's too much to cover this time, we'll table it for later
- Please ask a question whenever it comes up
    - Don't call out in the middle, just raise your hand and I'll call on you whenever a good time to break comes up
    - I check in with my audience a lot so I should be pretty quick in getting to your question
- If there's something you don't understand, please ask
    - You're probably not the only one
    - Even if you are, everyone would benefit from a reframing of the topic
- Image credits in accompanying Markdown file as image alttext

## Linguistics/Theoretical Background
### Information
- Shannon information/entropy/surprisal

:::::::::::::: {.columns}
::: {.column width="50%"}
![Shannon Entropy](shannon_entropy "https://en.wikipedia.org/wiki/Entropy_(information_theory)"){#id .class width=150 height=100px}
:::
::: {.column width="50%"}
![Binomial Entropy Function](shannon_entropy_graph "https://en.wikipedia.org/wiki/Entropy_(information_theory)#/media/File:Binary_entropy_plot.svg"){#id .class width=150 height=100px}
:::
::::::::::::::

### A brief history of time
- The "time sandwich" of language
    - TEMPORAL: segments (phonotactics) and speech (articulatory dynamics, etc.)
    - ATEMPORAL: morphemes and words and sentences are not really temporal (evidenced by free variation in word order, etc.)
    - TEMPORAL: discourse and dialog are temporal again (no language starts a story with "that's all folks")

- The validity of bidirectionality is thus questionable at levels that are temporal
    - Patient2vec redux: there's a reason we call people who had cancer but now don't in "remission" rather than "healthy"
    - BUT if you don't yet have cancer, we don't say you're in "premission"
    - Time is always marching forward so you have to be careful about the implications of modeling choices of uni-directional vs. bi-directional

### Speech (TEMPORAL) vs. Language (ATEMPORAL)
- Sublexical sound units
![Aligned Spectrograms](aligned_spectrogram "http://slideplayer.com/slide/5216736/16/images/40/Waveform+&+Spectrogram+aligned.jpg")
- Sublexical orthographic units = letters


### Speech vs. Language: Differences
- Variance in acoustics/articulation and invariance in orthography (except handwriting)
    - Sounds change by random and by rules
- Discrete time steps vs. continuous time steps (i.e. one is imperfectly sampled, the other perfectly captured)
    - Speech sampling, the sinc function, perfect reconstruction (with max fs/2)

:::::::::::::: {.columns}
::: {.column width="50%"}
![Sinc Equation](sinc_formula "https://en.wikipedia.org/wiki/Sinc_function"){#id .class width=150 height=100px}
:::
::: {.column width="50%"}
![Sinc Function](sinc_graph "https://upload.wikimedia.org/wikipedia/commons/4/4e/Sinc_function_%28unnormalized%29.svg"){#id .class width=150 height=100px}
:::
::::::::::::::

### Speech vs. Language: Similarities
- Both are arbitrary:

:::::::::::::: {.columns}
::: {.column width="50%"}
Sounds:
![Dwight's Folly](dwight_r "https://3.bp.blogspot.com/-lyNarefqBq0/UxcpBrUHUGI/AAAAAAAAYZk/Pg6OoKWmipE/s1600/murder.png"){#id .class width=200 height=200px}
(minus onomatopoeias)
:::
::: {.column width="50%"}
Letters:
<!-- ![letter origins]("http://www.designingwithtype.com/items/images/phoenician464.gif") -->
![Orthographic Evolution](orthographic_evolution "https://laughingsquid.com/wp-content/uploads/2019/01/Evolution-of-the-English-Alphabet.png"){#id .class width=250 height=200px}
:::
::::::::::::::

### Meaning et al. (ATEMPORAL)
- Syntax: word order freely varies across languages (SVO, OVS, SOV, etc. have same meaning)
- (Distributional) Semantics: locality matters
    - "You shall know a word by the company it keeps" - Firth, 1957
- Hierarchical vs. Linear (Spoiler: stack LSTM marries the 2)
    - Language is necessarily constrained to linearity given directionality in orthography and speech acts
    - But simultaneously has a rich hierarchical structure not necessarily obvious from the surface linear form
    - The hierarchical structure can change meaning even when linear order stays the same:
    ![Attachment Ambiguity](chart_parse_ambiguity "https://web.archive.org/web/20150221021038/http://webdocs.cs.ualberta.ca/~lindek/650/papers/chartParsing.pdf"){#id .class width=150 height=100px}
- Meaning is not really constrained by time, this makes things like Bidirectional LSTMs kosher

### Discourse and dialog (TEMPORAL)
- Propositions: a statement made that can be evaluated with a truth value (boolean logic)
- What happens when statements are not made in a vaccuum (interlocutor mental modeling of common ground, accommodation, etc.)?
- Gricean maxims:
<!-- from https://www.sas.upenn.edu/~haroldfs/dravling/grice.html -->
    1. Quantity: try to be as informative as one possibly can, and give as much information as is needed, and no more.
    2. Quality: try to be truthful, and not give information that is false or that is not supported by evidence.
    3. Relation: try to be relevant, and says things that are pertinent to the discussion.
    4. Manner: try to be as clear, as brief, and as orderly as you can in what one says, and avoid obscurity and ambiguity.
- How do you establish a common ground of exchange as a conversation group? (diapix, etc.)
- How does dialog (alternatively, narrative) evolve over time?


## Design questions
### How do we obtain continuous embeddings from discrete entities?
- Letters: how can we tell that vowels are more similar to each other than consonants (their ascii codes/vocabulary instances are categorical NOT ordinal)
- Words: how can we tell that some words are more related than others either syntactically (i.e. parts of speech) or semantically (i.e. thematically related) rather than just having a one-hot for the vocabulary word that is being encoded
- Phrase/sentences: theoretically infinite possible combinations of words, which while at the surface differ but semantically are very similar
    - "I sipped a cup of joe" V.
    - "He drank his mug of coffee"
- Documents: definitely infinite possibilities, but we still need to be able to cluster them based on sentiment, topic, etc. to be useful to extend to out of training instances (which will be almost all documents)

### What level of structure do you model text at?
- Letter/Subword?: fasttext (https://research.fb.com/fasttext/)
- Word?: Word2Vec (https://pypi.org/project/word2vec/)
- Phrase (a.k.a. constituent, e.g. noun phrase)?: RNTN (https://github.com/alsoltani/RNTN)
- Sentence?: Skip-thought vectors (https://github.com/ryankiros/skip-thoughts)
- Document?: TF-IDF, LDA, etc.


## (Some) common problems and why we care
### Machine translation
- Considering the 7k lanugages of the world (or at least the ~50 represented on the internet), it would be nice to not have language be a barrier to human exchange (shown to create insular web communities)
    - Google translate
    - Other boutique translation sites
    - Legal translations
    - Medical translations

### Sentiment analysis and emotion detection
- Given a language sample, we would like to detect how the speaker/writer feels about the topic they're under discussion (either well defined or estimated by topic modeling)
    - Product reviews (Amazon, Ebay, Yelp, Goodreads, etc.)
    - Chatbots (to react to user emotion: XiaoIce)
    - Reddit?
    - Suicide prevention on online forums/ from online presences

### Grammar parsing, grammar checking, POS (part of speech) tagging
- Given a language sample, we would like to be able to label all the words their correct parts of speech and build a syntactic model of the sentence
    - Aids in numerous downstream tasks
    - Grammar checking/advice (Grammarly, Microsoft Word)
    - Automatic role labeling (Subject, Object, etc.)

### Word sense disambiguation
- Given a homographic word (or homophonic if speech), which of the different meanings does the user wish to convey?
    - Image search (google, yahoo, etc.)
    - Machine translation (the same homograph may be translated differently, e.g. plant:

    ![Flora Plant](flora_plant){#id .class width=150 height=100px} VS. ![Factory Plant](factory_plant){#id .class width=150 height=100px}

    - Robotics (to navigate or retrieve objects correctly)
    - Conversation agents (to be able to glean meaning and respond sensically)
    - etc.

## Traditional approaches
### Background
- You will notice that many of these approaches leverage (big for the time) data and statistical analyses to try to solve the problem
- Useful to know as many are still used as baseline interpretable approaches when reporting SOTA findings (you should know what they're comparing against as a baseline)
- It's worth recognizing that the approach of training statistical models on big corpora is fairly old
- The statistical modeling methods of today are definitely better (even the "traditional" methods are still improving) but the fundamental approach has remained the same
- Whether you believe ML is "just statistics" or not, it certainly stems from stats, leverages its history, and borrows heavily from the discipline so while the ML approach is novel, it is not necessarily categorically different from old approaches

## Machine translation
### History
- Large interest post-WW2 as the success of decyphering codes by machines was a proven concept
- Initial approaches treated translation as a code to be broken (like the "enigma"), but that's not really how language works (e.g. Native American "Wind Talkers" in the war talking safely over tapped lines)
    - Also requires massive amount of human oversight to develop the grammatical rules
- Abandoned in the 60s due to overhype (sound like an AI story you've heard of?)
- Then statistical methods took the scene and with the ever growing amount of data (particularly parallel corpora, e.g. EUROPARL (EU), Hansard (CA)) took off

### Statistical machine translation (SMT)
- Uses both a language model $P(y)$ and translation model $P(y|x)$ models the probability that $y$ is a good translation for $x$
    - ![](SMT_Bayes "https://members.loria.fr/EGalbrun/resources/Gal09_phrase.pdf"){#id .class width=150 height=100px}
    - ![](SMT_argmax "https://members.loria.fr/EGalbrun/resources/Gal09_phrase.pdf"){#id .class width=150 height=100px}
- Language models (LMs) estimate the probability of an utterance $P(y) = P(w_1...w_n)$ in a target language based on a training corpus   
    - We still use LMs, just neural LMs for the most part
    - Traditional approaches to estimating probability of a sentence were built on word frequency (available going back to the 40s, used for TEFOL)
    - Then n-grams came along to better model linearity and dependencies
    - Then lots of n-gram tricks (e.g. backoff and skipgrams) were invented to be more robust to sparsity and provide better estimates for the true probability of the sentence


### SMT Cont.
- Translation models try to transform one sentence/phrase representation into another

:::::::::::::: {.columns}
::: {.column width="50%"}
- Can be done on syntax trees:
    ![](SMT_Crossing_Reordering_Fertility "https://www.isi.edu/~marcu/papers/cr_ghkm_naacl04.pdf"){#id .class width=100 height=100px}

:::
::: {.column width="50%"}
- Or done at the phrasal level (using a learned weighting on *parallelized* sentences called a "phrase table")
    ![](SMT_Softmax_features_weightings "https://members.loria.fr/EGalbrun/resources/Gal09_phrase.pdf"){#id .class width=150 height=150px}
:::
::::::::::::::

- Phrase tables address the issue of reordering (i.e. "alignment") and non-1-to-1 mappings of words (e.g. ne... pas) between biphrases (the translation pair) seen in the tree above
- Weights are learned using SGD on a feature mapping to maximize MAP of the translation weights between the biphrases

### Automatic evaluations of translation
- BLEU is most commonly used metric to automatically evaluate translation
- Takes clipped n-gram (clipping cannot exceed max n-gram count in any reference)
![](BLEU_modified_unigram "https://aclweb.org/anthology/P02-1040.pdf"){#id .class width=150 height=100px}
- Sums over n-grams and over candidates
![](BLEU2 "https://aclweb.org/anthology/P02-1040.pdf"){#id .class width=150 height=100px}
- Enforces brevity penalty (BP) where $r$ is best match (reference) length and $c$ is candidate length (summed over all sentences in the document)
![](BLEU_brevity_penalty "https://aclweb.org/anthology/P02-1040.pdf"){#id .class width=150 height=100px}
- Then sums over ngrams
![](BLEU_4_gram_uniform_weighting "https://aclweb.org/anthology/P02-1040.pdf"){#id .class width=150 height=100px}

## Text Classification
### Naive Bayes Classifier
- Came about in the 60s with the rise of statistical data-modeling (rather than rule-based) methods
- Uses words as a whole (discrete units) and as they are (untransformed representations, though preprocessing is possible: e.g. stemmers/lemmatizers) as features to classify the text
- Because it simply uses words (or n-grams), it's linear time to train (sounds nice...) by using a closed-form MLE to fit the classifier
- Reason it is called "naive" is that it assumes all features are independent in order to vastly simplify calculations, training time, complexity, etc. (definitely not true, but seems to not harm performance that much)
- Because of the model simplicity, it needs way less training data
- Easy to train, you just need to count words and count labeled documents, maybe use stop words (close-class words etc.)

### Naive Bayes Math
- Start with base equation
![](NB_formula "https://en.wikipedia.org/wiki/Naive_bayes")
- Ditch denomenator and assume independence
![](NB_independence_assumption "https://en.wikipedia.org/wiki/Naive_bayes")

### Naive Bayes Classification
- Choose the class that maximizes your likelihood
![](NB_argmax_classifier "https://en.wikipedia.org/wiki/Naive_bayes")
- In the multinomial case, this is a linear classifier
![](NB_multinomial "https://en.wikipedia.org/wiki/Naive_bayes")

## Grammar Parsing, etc.
### Chart Parsing
- Attempts to overcome the fact that parse trees are mutually exclusive and often cannot share information between them (they have completely different structures)
![](chart_parse_partial_structure "https://web.archive.org/web/20150221021038/http://webdocs.cs.ualberta.ca/~lindek/650/papers/chartParsing.pdf"){#id .class width=250 height=200px}
- Additionally, before they achieve closure, they're not really possible or meaningful to store as individual constituents
- Uses dynamic programming to build solutions that both can store different intermediary steps (taking care of pre-closure representations) and represent different paths to solutions (taking care of separate storage of ambiguity)

### Quick Dynamic Programming Review
![Dynamic Programming](dynamic_programming "https://image.slidesharecdn.com/5-150507111808-lva1-app6892/95/53-dynamic-programming-03-22-638.jpg?cb=1430997552")

### Chart Parsing Cont.
- Represents sentences as a DAG with word and POS arcs
![](chart_parse_graph_topology "https://web.archive.org/web/20150221021038/http://webdocs.cs.ualberta.ca/~lindek/650/papers/chartParsing.pdf"){#id .class width=400 height=100px}
- Allows incomplete parses in the chart
![](chart_parse_partial_parse "https://web.archive.org/web/20150221021038/http://webdocs.cs.ualberta.ca/~lindek/650/papers/chartParsing.pdf"){#id .class width=150 height=75px}
- Allows multiple correct parses to be stored in the graph
![](chart_parse_different_parses "https://web.archive.org/web/20150221021038/http://webdocs.cs.ualberta.ca/~lindek/650/papers/chartParsing.pdf"){#id .class width=400 height=100px}

## Word Sense Disambiguation
### WordNet
- Built in 1995 by George Miller and a bunch of other contributors by mining many corpora of English (usually built on word counts of literary published texts, e.g. Kucera and Francis)
- It's a lexical ontology database that's inspired by psycholinguistic theories about how lexical memory is structured (theory network, e.g. see Steven Pinker's Royal Institute YouTube talk), that gives rise to things like "word association" games
- Contains entry for all the senses of a given word and hyperlinks to synonyms and antonyms for each sense
- Also contains for nouns hypernyms and hyponyms (superclass/subclass), meronyms and holonyms (is part of/contains); for verbs hypernym, troponym (specific way of doing something), entailment (necessary precondition)

### WordNet UI
![](Wordnet_example "http://wordnetweb.princeton.edu/perl/webwn")

### WordNet Cont.
- In this way, it creates an undirected unweighted (or bivalent weighted if you include antonyms) graph network that represents word sense relationships
- Given this database, you can do the numerous analyses that are possible on graphs to learn relations in the network
- A simple approach might be to substitute in all the possible synonyms of each sense and see how many of those are attested in the corpus

## Neural Approaches:
### The problem
- Seeks to solve the vanishing/exploding gradient problem (don't they all...): i.e. given N layers to a network, the gradient is raised to the N by the time it gets back to the first layer
- This is particularly a problem for recurrent neural networks (RNNs) because back propogation through time (BPTT) means that the networks are as deep as their sequence is long. So given a sentence of N words, there are essentially N layers to the network (and that's assuming each time state only has 1 layer)
- In 1997 Hochreiter and Schmidhuber come up with a solution

### LSTM V1
- Introduces the constant error carousel (CEC)
    - Selectively allows error to flow back through the network for an arbitrarily long amount of time (kind of resnet-y)
    - Determined by gating functions that are concurrently trained/learned
    - Termed "gates" because they determine how much error flows back and how much memory flows forward, like the gates in water lochs
- How to solve the vanishing/exploding gradient?
    - Don't backpropogate (at least not all the way...)
    - Backprop only for the current time-step so it's a reasonable decay of gradient information, let the CEC take care of error propogation across timesteps
- Form "memory cell" units that have internal topologies but all receive the same input from the input gate and whose combined outputs are fed to the output gate

### LSTM Gates Semantics
- Forward Pass
    - Input gate protects memory (and in back-pass error) from irrelevant/undesirable inputs: how much incoming information (input) should I allow into my hidden layers (memory)?
    - Output gate protects downstream units from irrelevant memory contents (i.e. hidden states): how much hidden information (memory) should I pass downstream to influence future cells?
- Backward Pass
    - Output gate decides how much error to keep in the CEC to prevent vanishing/exploding error
    - Input gate decided how much error to throw away before passing it on to the previous time step/memory cell    

### LSTM Gates Syntax
![](LSTM_forget_and_vanilla_just_topology "https://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015"){#id .class width=320 height=180px}
![](LSTM_vanilla_equations "https://en.wikipedia.org/wiki/Long_short-term_memory"){#id .class width=320 height=180px}

### LSTM V2: Forgetting
- Rather than letting the error backpropogate unobstructed through the network, let the network decide what to let through the CEC
    - Add in forget gate that controls what is let through from the previous cell memory/hidden state

### Forgetting LSTM
![](LSTM_forget_topology "http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf"){#id .class width=320 height=180px}
![](LSTM_forget_equations "https://en.wikipedia.org/wiki/Long_short-term_memory"){#id .class width=320 height=180px}

### LSTM V3: Peephole
- Let the previous state of the memory cell directly influence what will be forgotten, let in, and let out
![](LSTM_peephole_topology_highlighted.png "https://en.wikipedia.org/wiki/Long_short-term_memory#/media/File:Peephole_Long_Short-Term_Memory.svg")

### LSTM V3: Peephole Cont.
![](LSTM_peephole_topology "https://ieeexplore.ieee.org/document/963769"){#id .class width=320 height=180px}
![](LSTM_peephole_equations "https://en.wikipedia.org/wiki/Long_short-term_memory"){#id .class width=320 height=180px}

### LSTM: Marrying semantics and syntax
![](LSTM_forget_topology_and_equations.jpg "https://slideplayer.com/slide/12364661/")

### LSTM V4: Coupled forget and input gates
- Rather than having separate input and forgetting gates, why not make them complements of each other
    - Get rid of input gate parameters and determine wholly from forget gate
    - Gives a tug-of-war relationship to information preservation vs. onboarding
![](coupled_forget_and_input "http://colah.github.io/posts/2015-08-Understanding-LSTMs/")

### Simplified LSTM: GRU
- Simplify, simplify, simplify (at the cost of power)
    - Combine forget and input gates (why stop at coupling them?) into "update gate" $z_t$
    - Do away with separate hidden state and cell memory and have just hidden state
    - Introduce a reset vector to allow "forgetting"
![](LSTM_GRU "http://colah.github.io/posts/2015-08-Understanding-LSTMs/")
